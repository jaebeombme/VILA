{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout = sys.__stdout__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from experts.expert_monai_brats import ExpertBrats\n",
    "from experts.expert_monai_vista3d import ExpertVista3D\n",
    "from experts.expert_torchxrayvision import ExpertTXRV\n",
    "from experts.expert_hd_glio import ExpertHDGLIO\n",
    "from llava.conversation import SeparatorStyle, conv_templates\n",
    "from llava.mm_utils import KeywordsStoppingCriteria, get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.model.builder import load_pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"/home/hufsaim/VLM/VLM/Llama3-VILA-M3-8B/snapshots/df60e0276e2ae10624c86dabe909847a03b2a5cb\"\n",
    "OUTPUT_PATH = \"/home/hufsaim/VLM/VLM/m3/demo/0527\"\n",
    "JSON_FILE = \"/home/hufsaim/VLM/VLM/data/0527/question1.json\"\n",
    "SLICE_SAVE_PATH = \"/home/hufsaim/VLM/VLM/m3/demo/sliced_images\"\n",
    "DATA_ROOT_DIR = \"/home/hufsaim/VLM/VLM/data/0527\"\n",
    "\n",
    "MODEL_CARDS = \"\"\"Here is a list of available expert models:\\n\n",
    "<BRATS(args)> \n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, \n",
    "Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, \n",
    "Valid args are: None\\n\n",
    "<VISTA3D(args)> \n",
    "Modality: CT, \n",
    "Task: segmentation, \n",
    "Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, \n",
    "Accuracy: 127 organs: 0.792 Dice on average, \n",
    "Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n\n",
    "<VISTA2D(args)> \n",
    "Modality: cell imaging, \n",
    "Task: segmentation, \n",
    "Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, \n",
    "Accuracy: Good accuracy across several cell imaging datasets, \n",
    "Valid args are: None\\n\n",
    "<CXR(args)> \n",
    "Modality: chest x-ray (CXR), \n",
    "Task: classification, \n",
    "Overview: pre-trained model which are trained on large cohorts of data, \n",
    "Accuracy: Good accuracy across several diverse chest x-rays datasets, \n",
    "Valid args are: None\\n\n",
    "<HD-Glio(args)>\n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A deep learning-based model designed for high-grade glioma segmentation in brain MR images. HD-Glio leverages ensemble 3D U-Net architectures and robust preprocessing including brain extraction, intensity normalization, and co-registration. It is tailored to identify and delineate tumor subregions (enhancing tumor, tumor core, and whole tumor) with high accuracy.\n",
    "Accuracy: Tumor core (TC): 0.860 - Whole tumor (WT): 0.910 - Enhancing tumor (ET): 0.800 - Average: 0.857\n",
    "Valid args are: None\\n\n",
    "Give the model <NAME(args)> when selecting a suitable expert model.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mri_paths(case_dir):\n",
    "    \"\"\"\n",
    "    Function to adjust the input order when inputting MRI data to pass it to the BraTS model input\n",
    "    \"\"\"\n",
    "    modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "    image_paths = []\n",
    "\n",
    "    for modality in modalities:\n",
    "        for ext in ('.nii', '.nii.gz'):\n",
    "            fname = next((f for f in os.listdir(case_dir) if f.lower().endswith(ext) and f\"{modality}.\" in f.lower()), None)\n",
    "            gt_name = str(f for f in os.listdir(case_dir) if f.lower().endswith(ext) and f\"{'seg'}.\" in f.lower())\n",
    "            if fname:\n",
    "                image_paths.append(os.path.join(case_dir, fname))\n",
    "                break\n",
    "\n",
    "    return image_paths if len(image_paths) == 4 else None, gt_name if gt_name else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_image(nifti_path, slice_axis=2):\n",
    "    \"\"\"\n",
    "    Convert nifti file to png for use as input for VILA-M3 model\n",
    "    \"\"\"\n",
    "    nifti_img = nib.load(nifti_path)\n",
    "    volume = nifti_img.get_fdata()\n",
    "\n",
    "    slice_idx = volume.shape[slice_axis] // 2\n",
    "    if slice_axis == 0:\n",
    "        slice_img = volume[slice_idx, :, :]\n",
    "    elif slice_axis == 1:\n",
    "        slice_img = volume[:, slice_idx, :]\n",
    "    else:\n",
    "        slice_img = volume[:, :, slice_idx]\n",
    "\n",
    "    slice_norm = (slice_img - np.min(slice_img)) / (np.max(slice_img) - np.min(slice_img))\n",
    "    slice_image = Image.fromarray(np.uint8(slice_norm * 255)).convert('RGB')\n",
    "\n",
    "    return slice_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_to_json(result, output_dir, base_filename=\"inference_result_with_expert\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    counter = 1\n",
    "\n",
    "    filename = f\"{base_filename}_{counter}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    while os.path.exists(filepath):\n",
    "        filename = f\"{base_filename}_{counter}.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        counter += 1\n",
    "\n",
    "    formatted_results = []\n",
    "\n",
    "    for sample_id, info_list in result.items():\n",
    "        combined = {\"id\": sample_id}\n",
    "        for entry in info_list:\n",
    "            combined.update(entry)\n",
    "        formatted_results.append(combined)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_json_data(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3Inference:\n",
    "    def __init__(self, model_path=MODEL_PATH, conv_mode=\"llama_3\"):\n",
    "        model_name = model_path.split(\"/\")[-1]\n",
    "        self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(model_path, model_name, device=\"cuda:0\")\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "    def inference(self, \n",
    "        image_path, \n",
    "        prompt,\n",
    "        max_tokens: int = 1024,\n",
    "        temperature: float = 0.0,\n",
    "        top_p: float = 0.9,\n",
    "        output_path: str = None\n",
    "    ):  \n",
    "        print(f'[DEBUG] Inferce')\n",
    "        answer = []\n",
    "        conv = conv_templates[self.conv_mode].copy()\n",
    "\n",
    "        answer.append({\"USER\" : prompt})\n",
    "        answer.append({\"image path\": image_path})\n",
    "\n",
    "        prompt_text = conv.get_prompt()\n",
    "\n",
    "        if isinstance(image_path, list):\n",
    "            images = []\n",
    "            for img_path in image_path:\n",
    "                if img_path.endswith(('.nii', '.nii.gz')):\n",
    "                    image = load_nifti_image(img_path)\n",
    "                else:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                images.append(image)\n",
    "            images_tensor = process_images(images, self.image_processor, self.model.config).to(self.model.device, dtype=torch.float16)\n",
    "\n",
    "            image_tokens = \" \".join([\"<image>\"] * len(images))\n",
    "            full_prompt = f\"{MODEL_CARDS}\\n{image_tokens}\\n{prompt}\"\n",
    "        else: \n",
    "            if image_path.endswith(('.nii', '.nii.gz')):\n",
    "                image = load_nifti_image(image_path)\n",
    "            else:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            images_tensor = process_images([image], self.image_processor, self.model.config).to(self.model.device, dtype=torch.float16)\n",
    "\n",
    "            full_prompt = f\"{MODEL_CARDS}\\n<image>\\n{prompt}\"\n",
    "\n",
    "        media_input = {\"image\": [img for img in images_tensor]} if images_tensor is not None else None\n",
    "        media_config = {\"image\": {}} if images_tensor is not None else {}\n",
    "\n",
    "        conv.append_message(conv.roles[0], full_prompt)\n",
    "        conv.append_message(conv.roles[1], \"\")\n",
    "\n",
    "        prompt_text = conv.get_prompt()\n",
    "        \n",
    "        input_ids = tokenizer_image_token(prompt_text, self.tokenizer, return_tensors=\"pt\").unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "        stopping_criteria = KeywordsStoppingCriteria([stop_str], self.tokenizer, input_ids)\n",
    "\n",
    "        # VILA-M3 Model Inference\n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                media=media_input,\n",
    "                media_config=media_config,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_tokens,\n",
    "                use_cache=True,\n",
    "                stopping_criteria=[stopping_criteria],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "        if output.endswith(stop_str):\n",
    "            output = output[:-len(stop_str)].strip()\n",
    "\n",
    "        # output = 'I segmented the tumor using <HD-GLIO()>.'\n",
    "        answer.append({\"VILA-M3\" : output})\n",
    "        \n",
    "        # Check Tirrger\n",
    "        expert_model = None\n",
    "        for expert_cls in [ExpertVista3D, ExpertTXRV, ExpertHDGLIO]:\n",
    "            expert = expert_cls()\n",
    "            if expert.mentioned_by(output):\n",
    "                expert_model = expert\n",
    "                break\n",
    "\n",
    "        expert_response, expert_image = None, None\n",
    "        if expert_model:\n",
    "            tqdm.write(f'[DEBUG] Expert Model Name : {expert_model.model_name}')\n",
    "            try:\n",
    "                if isinstance(image_path, list):\n",
    "                    expert_img_file = image_path\n",
    "                else:\n",
    "                    expert_img_file = [image_path]\n",
    "\n",
    "                if all(f.endswith((\".nii\", \".nii.gz\")) for f in expert_img_file):\n",
    "                    try:\n",
    "                        nib_img = nib.load(expert_img_file[0])\n",
    "                        shape = nib_img.shape\n",
    "                        if len(shape) == 3:\n",
    "                            slice_index = shape[2] // 2 \n",
    "                            tqdm.write(f\"[DEBUG] Auto-calculated slice_index: {slice_index}\")\n",
    "                        else:\n",
    "                            tqdm.write(f\"[WARNING] Unexpected shape {shape} for NIfTI file.\")\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f\"[WARNING] Failed to load NIfTI file for slice index: {e}\")\n",
    "                        slice_index = 77  # fallback\n",
    "                \n",
    "                # Expert Model Inference\n",
    "                tqdm.write(f'[DEBUG] Expert Model Inference')\n",
    "                expert_response, expert_image_path, instruction = expert_model.run(\n",
    "                    img_file=expert_img_file,\n",
    "                    image_url=expert_img_file,\n",
    "                    input=output,\n",
    "                    output_dir=output_path,\n",
    "                    slice_index=slice_index,\n",
    "                    prompt=prompt,\n",
    "                )\n",
    "\n",
    "                answer.append({\"Expert\" : expert_response})\n",
    "                tqdm.write(f'[DEBUG] Expert Output : {expert_response}')\n",
    "                if expert_image_path:\n",
    "                    answer.append({\"Expert Image Path\" : expert_image_path})\n",
    "                \n",
    "                if instruction:\n",
    "                    conv = conv_templates[self.conv_mode].copy()\n",
    "                    image_tokens = \"\\n\".join([\"<image>\"] * (len(image_path) if isinstance(image_path, list) else 1))\n",
    "                    updated_prompt = f\"{expert_response}\\n{instruction}\\n{image_tokens}\"\n",
    "                    conv.append_message(conv.roles[0], updated_prompt)\n",
    "                    conv.append_message(conv.roles[1], \"\")\n",
    "                    updated_prompt_text = conv.get_prompt()\n",
    "\n",
    "                    answer.append({\"Expert\": instruction})\n",
    "                    tqdm.write(f'[DEBUG] Expert Model Instruction : {instruction}')\n",
    "\n",
    "                    input_ids = tokenizer_image_token(updated_prompt_text, self.tokenizer, return_tensors=\"pt\").unsqueeze(0).to(self.model.device)\n",
    "\n",
    "                    # VILA-M3 Model Inference With Expert Model Output Image\n",
    "                    with torch.inference_mode():\n",
    "                        updated_output_ids = self.model.generate(\n",
    "                            input_ids,\n",
    "                            media=media_input,\n",
    "                            media_config=media_config,\n",
    "                            do_sample=True if temperature > 0 else False,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            max_new_tokens=max_tokens,\n",
    "                            use_cache=True,\n",
    "                            stopping_criteria=[stopping_criteria],\n",
    "                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        )\n",
    "                    output = self.tokenizer.batch_decode(updated_output_ids, skip_special_tokens=True)[0].strip()\n",
    "                    if output.endswith(stop_str):\n",
    "                        output = output[:-len(stop_str)].strip()\n",
    "\n",
    "                    answer.append({\"VILA-M3\": output})\n",
    "\n",
    "            except Exception as e:\n",
    "                tqdm.write('[DEBUG] Failed')\n",
    "                expert_response = f\"Expert model encountered an error: {e}\"\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(\n",
    "        image_dir=None, \n",
    "        modality=None, \n",
    "        prompt=\"What type of brain tumor is present in this MRI image? And if there is a tumor in this image, segment the tumor area.\",\n",
    "        pairs=None,\n",
    "        output_path=None,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "        model=None\n",
    "    ):\n",
    "    inference_result = {}\n",
    "    inference_model = model\n",
    "    print(3)\n",
    "\n",
    "    if pairs:\n",
    "        for pair in pairs:\n",
    "            case_id = pair[\"id\"]\n",
    "            image_path = pair[\"image_path\"]\n",
    "            question = pair.get(\"question\", prompt)\n",
    "            # 단일 path도 리스트로 감쌈\n",
    "            if isinstance(image_path, str):\n",
    "                image_path = [image_path]\n",
    "            inference_result[case_id] = inference_model.inference(image_path=image_path, prompt=question, output_path=output_path, temperature=temperature, top_p=top_p)\n",
    "\n",
    "    else:\n",
    "        def is_image_file(filename):\n",
    "            return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.nii', '.nii.gz'))\n",
    "\n",
    "        if isinstance(image_dir, list):\n",
    "            image_paths = [p for p in image_dir if is_image_file(p)]\n",
    "            if image_paths:\n",
    "                inference_result[\"multi_image_input\"] = inference_model.inference(\n",
    "                    image_path=image_paths, prompt=prompt, output_path=output_path\n",
    "                )\n",
    "\n",
    "        elif isinstance(image_dir, str) and os.path.isfile(image_dir) and is_image_file(image_dir):\n",
    "            # 단일 파일\n",
    "            image_paths = [image_dir]\n",
    "            inference_result[\"single_image\"] = inference_model.inference(\n",
    "                image_path=image_paths, prompt=prompt, output_path=output_path\n",
    "            )\n",
    "\n",
    "        elif isinstance(image_dir, str) and os.path.isdir(image_dir):\n",
    "            # 디렉토리 처리\n",
    "            for case_name in os.listdir(image_dir):\n",
    "                case_path = os.path.join(image_dir, case_name)\n",
    "                image_paths = []\n",
    "\n",
    "                if os.path.isfile(case_path) and is_image_file(case_name):\n",
    "                    image_paths = [case_path]\n",
    "\n",
    "                elif os.path.isdir(case_path):\n",
    "                    if modality and modality.upper() == 'MRI':\n",
    "                        image_paths, _ = collect_mri_paths(case_path)\n",
    "                    else:\n",
    "                        files = sorted(f for f in os.listdir(case_path) if is_image_file(f))\n",
    "                        image_paths = [os.path.join(case_path, f) for f in files]\n",
    "\n",
    "                if not image_paths:\n",
    "                    continue\n",
    "\n",
    "                print(f\"[DEBUG] Running inference on: {case_name}\")\n",
    "                inference_result[case_name] = inference_model.inference(\n",
    "                    image_path=image_paths, prompt=prompt, output_path=output_path, temperature=temperature, top_p=top_p\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(f\"[ERROR] Invalid image_dir: {image_dir}\")\n",
    "\n",
    "    return inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "if JSON_FILE:\n",
    "    data = load_json_data(JSON_FILE)\n",
    "\n",
    "    for sample in data:\n",
    "        sample_id = sample[\"id\"]\n",
    "        image_path = sample[\"img_path\"]\n",
    "        question = sample[\"question\"]\n",
    "\n",
    "        if isinstance(image_path, list):\n",
    "            image_path = [os.path.join(DATA_ROOT_DIR, p) if not os.path.isabs(p) else p for p in image_path]\n",
    "        elif isinstance(image_path, str):\n",
    "            if not os.path.isabs(image_path):\n",
    "                image_path = os.path.join(DATA_ROOT_DIR, image_path)\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected image_path type: {type(image_path)} (id: {sample_id})\")\n",
    "\n",
    "        pairs.append({\"id\": sample_id, \"image_path\": image_path, \"question\": question})\n",
    "else:\n",
    "    raise ValueError(\"JSON_FILE path must be specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb42de96ba843ec8e738f5c6ea0b72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = M3Inference(model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_batch_inference(pairs=pairs, model=model)\n",
    "save_result_to_json(results, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
