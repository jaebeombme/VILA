{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import KeywordsStoppingCriteria, get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is a list of available expert models:\\n\\n<BRATS(args)> \\nModality: MRI, \\nTask: segmentation, \\nOverview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, \\nAccuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, \\nValid args are: None\\n\\n<VISTA3D(args)> \\nModality: CT, \\nTask: segmentation, \\nOverview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, \\nAccuracy: 127 organs: 0.792 Dice on average, \\nValid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n\\n<VISTA2D(args)> \\nModality: cell imaging, \\nTask: segmentation, \\nOverview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, \\nAccuracy: Good accuracy across several cell imaging datasets, \\nValid args are: None\\n\\n<CXR(args)> \\nModality: chest x-ray (CXR), \\nTask: classification, \\nOverview: pre-trained model which are trained on large cohorts of data, \\nAccuracy: Good accuracy across several diverse chest x-rays datasets, \\nValid args are: None\\n\\n<HD-Glio(args)>\\nModality: MRI, \\nTask: segmentation, \\nOverview: A deep learning-based model designed for high-grade glioma segmentation in brain MR images. HD-Glio leverages ensemble 3D U-Net architectures and robust preprocessing including brain extraction, intensity normalization, and co-registration. It is tailored to identify and delineate tumor subregions (enhancing tumor, tumor core, and whole tumor) with high accuracy.\\nAccuracy: Tumor core (TC): 0.860 - Whole tumor (WT): 0.910 - Enhancing tumor (ET): 0.800 - Average: 0.857\\nValid args are: None\\n\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH=\"/home/hufsaim/VLM/VLM/m3/train/checkpoints/test3/checkpoint-798\"\n",
    "MODEL_BASE=None\n",
    "SOURCE=\"local\"\n",
    "\n",
    "OUTPUT_PATH = \"/home/hufsaim/VLM/VLM/m3/demo/0527\"\n",
    "JSON_FILE = \"/home/hufsaim/VLM/VLM/data/0527/question.json\"\n",
    "SLICE_SAVE_PATH = \"/home/hufsaim/VLM/VLM/m3/demo/sliced_images\"\n",
    "\n",
    "MODEL_CARDS = \"\"\n",
    "\n",
    "\"\"\"Here is a list of available expert models:\\n\n",
    "<BRATS(args)> \n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, \n",
    "Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, \n",
    "Valid args are: None\\n\n",
    "<VISTA3D(args)> \n",
    "Modality: CT, \n",
    "Task: segmentation, \n",
    "Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, \n",
    "Accuracy: 127 organs: 0.792 Dice on average, \n",
    "Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n\n",
    "<VISTA2D(args)> \n",
    "Modality: cell imaging, \n",
    "Task: segmentation, \n",
    "Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, \n",
    "Accuracy: Good accuracy across several cell imaging datasets, \n",
    "Valid args are: None\\n\n",
    "<CXR(args)> \n",
    "Modality: chest x-ray (CXR), \n",
    "Task: classification, \n",
    "Overview: pre-trained model which are trained on large cohorts of data, \n",
    "Accuracy: Good accuracy across several diverse chest x-rays datasets, \n",
    "Valid args are: None\\n\n",
    "<HD-Glio(args)>\n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A deep learning-based model designed for high-grade glioma segmentation in brain MR images. HD-Glio leverages ensemble 3D U-Net architectures and robust preprocessing including brain extraction, intensity normalization, and co-registration. It is tailored to identify and delineate tumor subregions (enhancing tumor, tumor core, and whole tumor) with high accuracy.\n",
    "Accuracy: Tumor core (TC): 0.860 - Whole tumor (WT): 0.910 - Enhancing tumor (ET): 0.800 - Average: 0.857\n",
    "Valid args are: None\\n\n",
    "Give the model <NAME(args)> when selecting a suitable expert model.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_image(nifti_path, slice_axis=2, sample_id=\"default\"):\n",
    "    try:\n",
    "        nifti_img = nib.load(nifti_path)\n",
    "        volume = nifti_img.get_fdata()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load NIfTI file: {nifti_path}, error: {e}\")\n",
    "\n",
    "    slice_idx = volume.shape[slice_axis] // 2\n",
    "\n",
    "    if slice_axis == 0:\n",
    "        slice_img = volume[slice_idx, :, :]\n",
    "    elif slice_axis == 1:\n",
    "        slice_img = volume[:, slice_idx, :]\n",
    "    else:\n",
    "        slice_img = volume[:, :, slice_idx]\n",
    "\n",
    "    slice_norm = (slice_img - np.min(slice_img)) / (np.max(slice_img) - np.min(slice_img) + 1e-8)\n",
    "    slice_image = Image.fromarray(np.uint8(slice_norm * 255)).convert('RGB')\n",
    "\n",
    "    os.makedirs(SLICE_SAVE_PATH, exist_ok=True)\n",
    "    save_path = os.path.join(SLICE_SAVE_PATH, f\"{sample_id}.png\")\n",
    "    slice_image.save(save_path)\n",
    "\n",
    "    return slice_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_to_json(result, output_dir, base_filename=\"inference_result_wo_expert\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    counter = 1\n",
    "\n",
    "    filepath = os.path.join(output_dir, f\"{base_filename}_{counter}.json\")\n",
    "    while os.path.exists(filepath):\n",
    "        filepath = os.path.join(output_dir, f\"{base_filename}_{counter}.json\")\n",
    "        counter += 1\n",
    "\n",
    "    formatted_results = []\n",
    "    for sample_id, info_list in result.items():\n",
    "        combined = {\"id\": sample_id}\n",
    "        for entry in info_list:\n",
    "            combined.update(entry)\n",
    "        formatted_results.append(combined)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def load_json_data(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3Inference:\n",
    "    def __init__(self, source=SOURCE, model_path=MODEL_PATH, conv_mode=\"llama_3\"):\n",
    "        self.source = source\n",
    "        if source == \"local\" or source == \"huggingface\":\n",
    "            # TODO: allow setting the device\n",
    "            self.conv_mode = conv_mode\n",
    "            if source == \"huggingface\":\n",
    "                model_path = snapshot_download(model_path)\n",
    "            model_name = get_model_name_from_path(model_path)\n",
    "            self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n",
    "                model_path, model_name, MODEL_BASE\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Source {source} is not supported.\")\n",
    "\n",
    "    def inference(self, image_path, prompt, sample_id=\"default\"):\n",
    "        answer = []\n",
    "        conv = conv_templates[self.conv_mode].copy()\n",
    "        answer.append({\"USER\": prompt})\n",
    "        answer.append({\"image path\": image_path})\n",
    "\n",
    "        if image_path.endswith(('.nii', '.nii.gz')):\n",
    "            image = load_nifti_image(image_path, sample_id=sample_id)\n",
    "        else:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        images_tensor = process_images([image], self.image_processor, self.model.config).to(self.model.device, dtype=torch.float16)\n",
    "        full_prompt = f\"{MODEL_CARDS}\\n<image>\\n{prompt}\"\n",
    "\n",
    "        media_input = {\"image\": [img for img in images_tensor]}\n",
    "        media_config = {\"image\": {}}\n",
    "\n",
    "        conv.append_message(conv.roles[0], full_prompt)\n",
    "        conv.append_message(conv.roles[1], \"\")\n",
    "        prompt_text = conv.get_prompt()\n",
    "        input_ids = tokenizer_image_token(prompt_text, self.tokenizer, return_tensors=\"pt\").unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "        stopping_criteria = KeywordsStoppingCriteria([stop_str], self.tokenizer, input_ids)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                media=media_input,\n",
    "                media_config=media_config,\n",
    "                max_new_tokens=1024,\n",
    "                temperature=0.0,\n",
    "                top_p=0.9,\n",
    "                use_cache=True,\n",
    "                stopping_criteria=[stopping_criteria],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "        if output.endswith(stop_str):\n",
    "            output = output[:-len(stop_str)].strip()\n",
    "\n",
    "        answer.append({\"VILA-M3\": output})\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(pairs):\n",
    "    inference_result = {}\n",
    "    inference_model = M3Inference()\n",
    "    for pair in tqdm(pairs, desc=\"Processing Cases\"):\n",
    "        sample_id = pair[\"id\"]\n",
    "        image_path = pair[\"image_path\"]\n",
    "        question = pair[\"question\"]\n",
    "\n",
    "        if isinstance(image_path, list):\n",
    "            image_path = image_path[0]\n",
    "\n",
    "        inference_result[sample_id] = inference_model.inference(\n",
    "            image_path=image_path,\n",
    "            prompt=question,\n",
    "            sample_id=sample_id\n",
    "        )\n",
    "\n",
    "    return inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79d0808ba614764b2ec95eb0ed898a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Processing Cases:   0%|          | 0/40 [00:00<?, ?it/s]/home/hufsaim/anaconda3/envs/vila/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/hufsaim/anaconda3/envs/vila/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Processing Cases: 100%|██████████| 40/40 [00:07<00:00,  5.10it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT_DIR = \"/home/hufsaim/VLM/VLM/data/0527\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if JSON_FILE:\n",
    "        data = load_json_data(JSON_FILE)\n",
    "        pairs = []\n",
    "\n",
    "        for sample in data:\n",
    "            sample_id = sample[\"id\"]\n",
    "            image_path = sample[\"image path\"]\n",
    "            question = sample[\"USER\"]\n",
    "\n",
    "            if isinstance(image_path, list):\n",
    "                image_path = [os.path.join(DATA_ROOT_DIR, p) if not os.path.isabs(p) else p for p in image_path]\n",
    "            elif isinstance(image_path, str):\n",
    "                if not os.path.isabs(image_path):\n",
    "                    image_path = os.path.join(DATA_ROOT_DIR, image_path)\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected image_path type: {type(image_path)} (id: {sample_id})\")\n",
    "\n",
    "            pairs.append({\"id\": sample_id, \"image_path\": image_path, \"question\": question})\n",
    "\n",
    "        results = run_batch_inference(pairs=pairs)\n",
    "        save_result_to_json(results, OUTPUT_PATH)\n",
    "    else:\n",
    "        raise ValueError(\"JSON_FILE path must be specified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
