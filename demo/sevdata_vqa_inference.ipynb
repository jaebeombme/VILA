{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from experts.expert_monai_brats import ExpertBrats\n",
    "from experts.expert_monai_vista3d import ExpertVista3D\n",
    "from experts.expert_torchxrayvision import ExpertTXRV\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import KeywordsStoppingCriteria, process_images, tokenizer_image_token\n",
    "from llava.conversation import conv_templates, SeparatorStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"MONAI/Llama3-VILA-M3-8B\"\n",
    "OUTPUT_PATH = \"/home/hufsaim/VLM/VLM/m3/demo/result_new\"\n",
    "JSON_FILE = \"/home/hufsaim/VLM/VLM/m3/demo/result_new/dataset_betcheck.json\"\n",
    "\n",
    "\n",
    "MODEL_CARDS = \"\"\"Here is a list of available expert models:\\n\n",
    "<BRATS(args)> \n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, \n",
    "Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, \n",
    "Valid args are: None\\n\n",
    "<VISTA3D(args)> \n",
    "Modality: CT, \n",
    "Task: segmentation, \n",
    "Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, \n",
    "Accuracy: 127 organs: 0.792 Dice on average, \n",
    "Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n\n",
    "<VISTA2D(args)> \n",
    "Modality: cell imaging, \n",
    "Task: segmentation, \n",
    "Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, \n",
    "Accuracy: Good accuracy across several cell imaging datasets, \n",
    "Valid args are: None\\n\n",
    "<CXR(args)> \n",
    "Modality: chest x-ray (CXR), \n",
    "Task: classification, \n",
    "Overview: pre-trained model which are trained on large cohorts of data, \n",
    "Accuracy: Good accuracy across several diverse chest x-rays datasets, \n",
    "Valid args are: None\\n\n",
    "<HD-Glio(args)>\n",
    "Modality: MRI, \n",
    "Task: segmentation, \n",
    "Overview: A deep learning-based model designed for high-grade glioma segmentation in brain MR images. HD-Glio leverages ensemble 3D U-Net architectures and robust preprocessing including brain extraction, intensity normalization, and co-registration. It is tailored to identify and delineate tumor subregions (enhancing tumor, tumor core, and whole tumor) with high accuracy.\n",
    "Accuracy: Tumor core (TC): 0.860 - Whole tumor (WT): 0.910 - Enhancing tumor (ET): 0.800 - Average: 0.857\n",
    "Valid args are: None\\n\n",
    "Give the model <NAME(args)> when selecting a suitable expert model.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti_image(nifti_path, slice_axis=2, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Load NIfTI file and extract 2D slice image for VILA-M3 input.\n",
    "    Automatically handles out-of-bounds slice indices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nifti_img = nib.load(nifti_path)\n",
    "        volume = nifti_img.get_fdata()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load NIfTI file: {nifti_path}, error: {e}\")\n",
    "\n",
    "    max_idx = volume.shape[slice_axis] - 1\n",
    "\n",
    "    # 슬라이스 인덱스 범위 검사 & 보정\n",
    "    if slice_idx is None or not (0 <= slice_idx <= max_idx):\n",
    "        print(f\"[WARNING] slice_idx {slice_idx} is out of bounds (max: {max_idx}). Using middle slice instead.\")\n",
    "        slice_idx = max_idx // 2\n",
    "\n",
    "    # 슬라이싱\n",
    "    if slice_axis == 0:\n",
    "        slice_img = volume[slice_idx, :, :]\n",
    "    elif slice_axis == 1:\n",
    "        slice_img = volume[:, slice_idx, :]\n",
    "    else:\n",
    "        slice_img = volume[:, :, slice_idx]\n",
    "\n",
    "    # 정규화 및 RGB 변환\n",
    "    slice_norm = (slice_img - np.min(slice_img)) / (np.max(slice_img) - np.min(slice_img) + 1e-8)\n",
    "    slice_image = Image.fromarray(np.uint8(slice_norm * 255)).convert('RGB')\n",
    "\n",
    "    return slice_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_to_json(result, output_dir, base_filename=\"inference_result\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    filename = f\"{base_filename}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    counter = 1\n",
    "    while os.path.exists(filepath):\n",
    "        filename = f\"{base_filename}_{counter}.json\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        counter += 1\n",
    "\n",
    "    formatted_results = []\n",
    "\n",
    "    for sample_id, info_list in result.items():\n",
    "        combined = {\"id\": sample_id}\n",
    "        for entry in info_list:\n",
    "            combined.update(entry)\n",
    "        formatted_results.append(combined)\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_json_data(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3Inference:\n",
    "    def __init__(self, model_path=\"MONAI/Llama3-VILA-M3-8B\", conv_mode=\"llama_3\"):\n",
    "        model_name = model_path.split(\"/\")[-1]\n",
    "        self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(model_path, model_name, device=\"cuda:0\")\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "    def inference(self, image_path, prompt, slice_index=None, max_tokens=1024, temperature=0.0, top_p=0.9):\n",
    "        answer = []\n",
    "        conv = conv_templates[self.conv_mode].copy()\n",
    "\n",
    "        answer.append({\"USER\": prompt})\n",
    "        answer.append({\"image path\": image_path})\n",
    "\n",
    "        if image_path.endswith(('.nii', '.nii.gz')):\n",
    "            image = load_nifti_image(image_path, slice_axis=2, slice_idx=slice_index)\n",
    "        else:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        images_tensor = process_images([image], self.image_processor, self.model.config).to(self.model.device, dtype=torch.float16)\n",
    "        full_prompt = f\"{MODEL_CARDS}\\n<image>\\n{prompt}\"\n",
    "\n",
    "        media_input = {\"image\": [img for img in images_tensor]}\n",
    "        media_config = {\"image\": {}}\n",
    "\n",
    "        conv.append_message(conv.roles[0], full_prompt)\n",
    "        conv.append_message(conv.roles[1], \"\")\n",
    "\n",
    "        prompt_text = conv.get_prompt()\n",
    "        input_ids = tokenizer_image_token(prompt_text, self.tokenizer, return_tensors=\"pt\").unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "        stopping_criteria = KeywordsStoppingCriteria([stop_str], self.tokenizer, input_ids)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                media=media_input,\n",
    "                media_config=media_config,\n",
    "                do_sample=(temperature > 0),\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_tokens,\n",
    "                use_cache=True,\n",
    "                stopping_criteria=[stopping_criteria],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        output = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "        if output.endswith(stop_str):\n",
    "            output = output[:-len(stop_str)].strip()\n",
    "\n",
    "        answer.append({\"VILA-M3\": output})\n",
    "\n",
    "        expert_model = None\n",
    "        for expert_cls in [ExpertBrats, ExpertVista3D, ExpertTXRV]:\n",
    "            expert = expert_cls()\n",
    "            if expert.mentioned_by(output):\n",
    "                expert_model = expert\n",
    "                break\n",
    "\n",
    "        if expert_model:\n",
    "            try:\n",
    "                expert_img_file = [image_path]\n",
    "\n",
    "                if slice_index is None and image_path.endswith(('.nii', '.nii.gz')):\n",
    "                    try:\n",
    "                        nib_img = nib.load(expert_img_file[0])\n",
    "                        shape = nib_img.shape\n",
    "                        if len(shape) == 3:\n",
    "                            slice_index = shape[2] // 2\n",
    "                            print(f\"[DEBUG] Auto-calculated slice_index: {slice_index}\")\n",
    "                        else:\n",
    "                            print(f\"[WARNING] Unexpected shape {shape} for NIfTI file.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARNING] Failed to load NIfTI file for slice index: {e}\")\n",
    "                        slice_index = 77  # fallback\n",
    "\n",
    "                expert_response, expert_image_path, instruction = expert_model.run(\n",
    "                    image_url=expert_img_file,\n",
    "                    input=output,\n",
    "                    output_dir=\"/home/hufsaim/VLM/VLM/m3/demo/expert_result\",\n",
    "                    img_file=expert_img_file,\n",
    "                    slice_index=slice_index,\n",
    "                    prompt=prompt,\n",
    "                )\n",
    "\n",
    "                answer.append({\"Expert\": expert_response})\n",
    "                if expert_image_path:\n",
    "                    answer.append({\"Expert Image Path\": expert_image_path})\n",
    "\n",
    "                if instruction:\n",
    "                    conv = conv_templates[self.conv_mode].copy()\n",
    "                    image_tokens = \"<image>\"\n",
    "                    updated_prompt = f\"{expert_response}\\n{instruction}\\n{image_tokens}\"\n",
    "                    conv.append_message(conv.roles[0], updated_prompt)\n",
    "                    conv.append_message(conv.roles[1], \"\")\n",
    "                    updated_prompt_text = conv.get_prompt()\n",
    "\n",
    "                    answer.append({\"Expert\": instruction})\n",
    "\n",
    "                    input_ids = tokenizer_image_token(updated_prompt_text, self.tokenizer, return_tensors=\"pt\").unsqueeze(0).to(self.model.device)\n",
    "\n",
    "                    with torch.inference_mode():\n",
    "                        updated_output_ids = self.model.generate(\n",
    "                            input_ids,\n",
    "                            media=media_input,\n",
    "                            media_config=media_config,\n",
    "                            do_sample=(temperature > 0),\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            max_new_tokens=max_tokens,\n",
    "                            use_cache=True,\n",
    "                            stopping_criteria=[stopping_criteria],\n",
    "                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        )\n",
    "\n",
    "                    output = self.tokenizer.batch_decode(updated_output_ids, skip_special_tokens=True)[0].strip()\n",
    "                    if output.endswith(stop_str):\n",
    "                        output = output[:-len(stop_str)].strip()\n",
    "\n",
    "                    answer.append({\"VILA-M3\": output})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Expert model encountered an error: {e}\")\n",
    "\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(pairs):\n",
    "    \"\"\"\n",
    "    Run batch inference using VILA-M3 with JSON-based input pairs.\n",
    "    If image_path is a list, only the first file will be used.\n",
    "\n",
    "    Args:\n",
    "        pairs (list of dict): List with keys 'id', 'image_path', 'question', optional 'slice_index'.\n",
    "        model_path (str): Path to the pretrained model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Inference results keyed by sample ID.\n",
    "    \"\"\"\n",
    "    inference_result = {}\n",
    "    inference_model = M3Inference(model_path=MODEL_PATH)  # VILA-M3 모델 로딩\n",
    "\n",
    "    for pair in tqdm(pairs, desc=\"Processing Cases\",leave=True):\n",
    "        sample_id = pair[\"id\"]  # 샘플 ID\n",
    "        image_path = pair[\"image_path\"]  # 이미지 경로 (list or str)\n",
    "        question = pair[\"question\"]  # 프롬프트\n",
    "        slice_index = pair.get(\"slice_index\", None)  # 선택적 slice index\n",
    "\n",
    "        # image_path가 list면 첫 번째 파일만 사용\n",
    "        if isinstance(image_path, list):\n",
    "            image_path = image_path[0]\n",
    "\n",
    "        # inference 호출 (slice_index 전달)\n",
    "        inference_result[sample_id] = inference_model.inference(\n",
    "            image_path=image_path,\n",
    "            prompt=question,\n",
    "            slice_index=slice_index\n",
    "        )\n",
    "\n",
    "    return inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = \"/home/hufsaim/VLM/data/for_test\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    if JSON_FILE:\n",
    "        data = load_json_data(JSON_FILE)  # JSON 로드\n",
    "        pairs = []\n",
    "\n",
    "        for sample in data:\n",
    "            sample_id = sample[\"id\"]\n",
    "            image_path = sample[\"image_path\"]\n",
    "            question = sample[\"question\"]\n",
    "            slice_index = sample.get(\"slice_index\", None)  # slice_index가 있으면 반영\n",
    "\n",
    "            # image_path가 list인지 str인지에 따라 절대경로 변환\n",
    "            if isinstance(image_path, list):\n",
    "                image_path = [\n",
    "                    os.path.join(DATA_ROOT_DIR, path) if not os.path.isabs(path) else path\n",
    "                    for path in image_path\n",
    "                ]\n",
    "            elif isinstance(image_path, str):\n",
    "                if not os.path.isabs(image_path):\n",
    "                    image_path = os.path.join(DATA_ROOT_DIR, image_path)\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected image_path type: {type(image_path)} (id: {sample_id})\")\n",
    "\n",
    "            pairs.append({\n",
    "                \"id\": sample_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"question\": question,\n",
    "                \"slice_index\": slice_index  # 있으면 전달\n",
    "            })\n",
    "\n",
    "        # run_batch_inference는 이제 pairs만 받음\n",
    "        results = run_batch_inference(pairs=pairs)\n",
    "\n",
    "        # 결과 저장\n",
    "        save_result_to_json(results, OUTPUT_PATH)\n",
    "    else:\n",
    "        raise ValueError(\"JSON_FILE path must be specified.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
